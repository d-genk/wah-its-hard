{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 35-entity-linker\n",
    ">Facilitating model training using spaCy's entity linking functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose**  This notebook contains code that accomplishes the following tasks:\n",
    "1. [Building a knowledge base](#Building the knowledge base) - read data exported from the Spatial Historian to build a knowledge base containing all entities in a given corpus\n",
    "2. [Building training data](#Building training data for model with entity linker) - convert data exported from the Spatial Historian to the specific format required to train an entity linking model\n",
    "\n",
    "See https://spacy.io/usage/training#entity-linker for more on spaCy's entity linking functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp ent_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "from ssda.collate import *\n",
    "from ssda.entity_corpus import *\n",
    "from ssda.xml_parser import *\n",
    "from ssda.add_ent import *\n",
    "from ssda.split_data import *\n",
    "import spacy\n",
    "from spacy.kb import KnowledgeBase\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below take three pieces of input from the Spatial Historian (a csv containing all of the people who appear in a specific volume, a csv linking these people to events described in that volume, and an xml file containing the full transcription of the volume) and produce a knowledge base that can be attached to a spaCy NLP object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four functions below (`build_kb_seed`, `generate_descriptions`, `generate_altnames`, and `build_aliases`) compartmentalize various tasks required to build the knowledge base and are subsequently combined to perform the build in `create_kb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def build_kb_seed(peopleCSV, ppeCSV):\n",
    "    '''\n",
    "    Parse Spatial Historian csvs\n",
    "        peopleCSV: a csv containing all people who appear in a given volume\n",
    "        ppeCSV: a csv linking these people to specific events\n",
    "\n",
    "        returns: lists of identifiers, names, references, and frequency counts for each person\n",
    "    '''\n",
    "    \n",
    "    #extract identifiers and names from peopleCSV\n",
    "    with open(peopleCSV, encoding=\"utf-8\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        ids = []\n",
    "        names = []\n",
    "        first = True\n",
    "        for row in csvreader:\n",
    "            if first:\n",
    "                first = False\n",
    "                continue\n",
    "            ids.append(row[0])\n",
    "            names.append(row[1])\n",
    "\n",
    "    #extract folio numbers and event attendees from ppeCSV\n",
    "    with open(ppeCSV, encoding=\"utf-8\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        sources = []\n",
    "        people = []\n",
    "        first = True\n",
    "        for row in csvreader:\n",
    "            if first:\n",
    "                first = False\n",
    "                continue\n",
    "            sources.append(row[1])       \n",
    "            ppl = []\n",
    "            ppl.append(row[3])\n",
    "            for attendee in row[4].split(';'):\n",
    "                person = attendee[attendee.find(\"P0\"):]\n",
    "                if person not in ppl:\n",
    "                    ppl.append(person)\n",
    "            for other in row[5].split(';'):\n",
    "                person = other[other.find(\"P0\"):]\n",
    "                if (other != '') and (person not in ppl):                   \n",
    "                    ppl.append(person)\n",
    "            people.append(ppl)\n",
    "\n",
    "    #finds the first reference to each person in the corpus and counts to total number of events that each person appears in   \n",
    "    refs = []\n",
    "    freqs = [0] * len(ids)\n",
    "\n",
    "    for j in range(len(ids)):\n",
    "        ref = False\n",
    "        for i in range(len(people)):\n",
    "            if ids[j] in people[i]:\n",
    "                freqs[j] += 1\n",
    "                if not ref:\n",
    "                    ref = True\n",
    "                    refs.append(sources[i])    \n",
    "    \n",
    "    return ids, names, refs, freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def generate_descriptions(names, refs, xml_df):\n",
    "    '''\n",
    "    Generates descriptions for each person\n",
    "        names: a list of names from build_kb_seed\n",
    "        refs: a list of references from build_kb_seed\n",
    "        xml_df: dataframe built by the parse_xml function from ssda.xml_parser\n",
    "\n",
    "        returns: lists of descriptions for each person\n",
    "    '''\n",
    "    \n",
    "    descriptions = []\n",
    "    #converts dataframe to lists\n",
    "    volume_ids, volume_titles, folio_ids, entry_numbers, entry_texts = parseXML(xml_df)\n",
    "    \n",
    "    #uses list of references to locate entry text from the dataframe and locates the first sentence containing a reference\n",
    "    #to the desired individual\n",
    "    for i in range(len(refs)):\n",
    "        trim = refs[i][refs[i].find('-') + 1:]\n",
    "        #drops leading zeros on identifiers\n",
    "        while trim[0] == '0':\n",
    "            trim = trim[1:]\n",
    "        found = False\n",
    "        for j in range(len(entry_texts)):\n",
    "            if found == True:\n",
    "                break\n",
    "            if folio_ids[j] == trim:\n",
    "                if names[i] in entry_texts[j]:\n",
    "                    #rudimentary fix for names containing periods (which they shouldn't)\n",
    "                    if '.' in names[i]:\n",
    "                        found = True                        \n",
    "                        descriptions.append(entry_texts[j])\n",
    "                        break\n",
    "                    sentences = entry_texts[j].split('.')\n",
    "                    #finds specific sentence\n",
    "                    for sentence in sentences:\n",
    "                        if names[i] in sentence:\n",
    "                            found = True\n",
    "                            if sentence[len(sentence) - 1] == ' ':\n",
    "                                sentence = sentence[:-1]\n",
    "                            descriptions.append(sentence + '.')\n",
    "                            break\n",
    "        \n",
    "        #in the first pass, approximately 20% of names didn't find a match b/c they were manipulated in some way\n",
    "        #between the verbatim entry text and being ingested into the Spatial Historian (e.g. last names added)\n",
    "        #this loop attempts to address that\n",
    "        if found == False:           \n",
    "            alt_names = []\n",
    "            name_parts = names[i].split(' ')\n",
    "            #missing parts of compound name (or multiple characters as #)\n",
    "            for name in name_parts:\n",
    "                alt_names.append(name)            \n",
    "            #individual characters replaced by #\n",
    "            for k in range(1, len(names[i])):\n",
    "                alt_names.append((names[i][:k]) + '#' + names[i][k + 1:])\n",
    "            #individual characters replaced by ' '\n",
    "            for l in range(1, len(names[i])):\n",
    "                alt_names.append((names[i][:l]) + ' ' + names[i][l:])\n",
    "            #names not capitalized\n",
    "            for m in range(len(name_parts)):\n",
    "                if name_parts[m][0].isupper():\n",
    "                    name_parts[m] = name_parts[m][0].lower() + name_parts[m][1:]\n",
    "            compound = name_parts[0]\n",
    "            alt_names.append(compound)\n",
    "            for k in range(1,len(name_parts)):\n",
    "                compound += ' ' + name_parts[k]\n",
    "                alt_names.append(compound)\n",
    "            #remove #s from name\n",
    "            if names[i].find('#') != -1:\n",
    "                no_pound = names[i].replace('#', '')\n",
    "                alt_names.append(no_pound)\n",
    "                for np in no_pound.split(' '):\n",
    "                    alt_names.append(np)\n",
    "            #check for all possible alternate names\n",
    "            for alt_name in alt_names:\n",
    "                for j in range(len(entry_texts)):\n",
    "                    if found == True:\n",
    "                        break\n",
    "                    if folio_ids[j] == trim:\n",
    "                        if alt_name in entry_texts[j]:\n",
    "                            sentences = entry_texts[j].split('.')\n",
    "                            for sentence in sentences:\n",
    "                                if alt_name in sentence:\n",
    "                                    found = True\n",
    "                                    sentence = sentence.replace(alt_name, names[i])\n",
    "                                    if sentence[len(sentence) - 1] == ' ':\n",
    "                                        sentence = sentence[:-1]\n",
    "                                    descriptions.append(sentence + '.')\n",
    "                                    break\n",
    "                                    \n",
    "        #this occurs if the name was completely illegible in the original\n",
    "        if \"Unknown\" in names[i]:            \n",
    "            descriptions.append(\"Who knows???\")\n",
    "            found = True\n",
    "            \n",
    "        #this should not happen\n",
    "        if found == False:\n",
    "            print(\"Failed to find a description for \" + names[i] + \" in \" + refs[i])\n",
    "                            \n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit testing: `build_kb_seed` and `generate_descriptions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests these functions using data from both volumes in our initial sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>desc</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P009-001234</td>\n",
       "      <td>Número 1 Pablo Ayende Maria Josefa Gomés En la...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P009-001235</td>\n",
       "      <td>Número 1 Pablo Ayende Maria Josefa Gomés En la...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P009-001236</td>\n",
       "      <td>Número 1 Pablo Ayende Maria Josefa Gomés En la...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P009-001237</td>\n",
       "      <td>Número 1 Pablo Ayende Maria Josefa Gomés En la...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P009-001238</td>\n",
       "      <td>Número 1 Pablo Ayende Maria Josefa Gomés En la...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P009-001239</td>\n",
       "      <td>Número 1 Pablo Ayende Maria Josefa Gomés En la...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P009-001240</td>\n",
       "      <td>Número 1 Pablo Ayende Maria Josefa Gomés En la...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P009-001241</td>\n",
       "      <td>Número 1 Pablo Ayende Maria Josefa Gomés En la...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P009-001242</td>\n",
       "      <td>Número 1 Pablo Ayende Maria Josefa Gomés En la...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P009-001243</td>\n",
       "      <td>Número 1 Pablo Ayende Maria Josefa Gomés En la...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               desc  freq\n",
       "0  P009-001234  Número 1 Pablo Ayende Maria Josefa Gomés En la...    47\n",
       "1  P009-001235  Número 1 Pablo Ayende Maria Josefa Gomés En la...     1\n",
       "2  P009-001236  Número 1 Pablo Ayende Maria Josefa Gomés En la...     1\n",
       "3  P009-001237  Número 1 Pablo Ayende Maria Josefa Gomés En la...     1\n",
       "4  P009-001238  Número 1 Pablo Ayende Maria Josefa Gomés En la...     1\n",
       "5  P009-001239  Número 1 Pablo Ayende Maria Josefa Gomés En la...     1\n",
       "6  P009-001240  Número 1 Pablo Ayende Maria Josefa Gomés En la...     7\n",
       "7  P009-001241  Número 1 Pablo Ayende Maria Josefa Gomés En la...     1\n",
       "8  P009-001242  Número 1 Pablo Ayende Maria Josefa Gomés En la...     1\n",
       "9  P009-001243  Número 1 Pablo Ayende Maria Josefa Gomés En la...     1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "ids, names, refs, freqs = build_kb_seed(\"FourPeople.csv\",\"FourPeoplePerEntry.csv\")\n",
    "descriptions = generate_descriptions(names, refs, parse_xml(\"four.xml\"))\n",
    "\n",
    "#knowledge base structure required by spaCy\n",
    "kb_input = {\"id\": ids, \"desc\": descriptions, \"freq\": freqs}\n",
    "\n",
    "#convert to dataframe for visual inspection\n",
    "kb_df = pd.DataFrame(kb_input)\n",
    "kb_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>desc</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P009-001522</td>\n",
       "      <td>Partida 1a Francisca Bentura Lunes veinte y oc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P009-001523</td>\n",
       "      <td>Partida 1a Francisca Bentura Lunes veinte y oc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P009-001524</td>\n",
       "      <td>Partida 1a Francisca Bentura Lunes veinte y oc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P009-001525</td>\n",
       "      <td>Partida 1a Francisca Bentura Lunes veinte y oc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P009-001526</td>\n",
       "      <td>Partida 1a Francisca Bentura Lunes veinte y oc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P009-001562</td>\n",
       "      <td>Partida 2a Maria Ysabel  Martes veinte y nueve...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P009-001563</td>\n",
       "      <td>Partida 1a Francisca Bentura Lunes veinte y oc...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P009-001564</td>\n",
       "      <td>Partida 2a Maria Ysabel  Martes veinte y nueve...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P009-001565</td>\n",
       "      <td>Partida 2a Maria Ysabel  Martes veinte y nueve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P009-001579</td>\n",
       "      <td>Partida 3 José Pantaleon Jueves treinta y uno ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               desc  freq\n",
       "0  P009-001522  Partida 1a Francisca Bentura Lunes veinte y oc...     2\n",
       "1  P009-001523  Partida 1a Francisca Bentura Lunes veinte y oc...     2\n",
       "2  P009-001524  Partida 1a Francisca Bentura Lunes veinte y oc...     2\n",
       "3  P009-001525  Partida 1a Francisca Bentura Lunes veinte y oc...     2\n",
       "4  P009-001526  Partida 1a Francisca Bentura Lunes veinte y oc...     1\n",
       "5  P009-001562  Partida 2a Maria Ysabel  Martes veinte y nueve...     2\n",
       "6  P009-001563  Partida 1a Francisca Bentura Lunes veinte y oc...     7\n",
       "7  P009-001564  Partida 2a Maria Ysabel  Martes veinte y nueve...     2\n",
       "8  P009-001565  Partida 2a Maria Ysabel  Martes veinte y nueve...     1\n",
       "9  P009-001579  Partida 3 José Pantaleon Jueves treinta y uno ...     2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "ids, names, refs, freqs = build_kb_seed(\"SevenPeople.csv\",\"SevenPeoplePerEntry.csv\")\n",
    "descriptions = generate_descriptions(names, refs, parse_xml(\"seven.xml\"))\n",
    "kb_input = {\"id\": ids, \"desc\": descriptions, \"freq\": freqs}\n",
    "kb_df = pd.DataFrame(kb_input)\n",
    "kb_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a list of names and returns a list of lists in which each element contains the corresponding name in the input list as well as any other name in the input list that is a substring of that name or that that name is a substring of. It's used to generate aliases, as well as prior probabilities for those aliases, for the knowledge base. This is obviously a very crude way of building this synonym list, and can/should be refined if the entity linker yields meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def generate_altnames(names):\n",
    "    '''\n",
    "    Generates a list of alternate names for each person\n",
    "        names: a list of names from build_kb_seed        \n",
    "\n",
    "        returns: a list of alternate names for each person\n",
    "    '''\n",
    "    \n",
    "    altnames = []\n",
    "    \n",
    "    for i in range(len(names)):\n",
    "        alts = [names[i]]\n",
    "        for j in range(len(names)):\n",
    "            if ((names[j] in names[i]) or (names[i] in names[j])) and (names[j] != names[i]):\n",
    "                alts.append(names[j])\n",
    "        altnames.append(alts)\n",
    "        \n",
    "    return altnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes the synonym list built by generate_altnames as well as the ID list from build_kb_seed and returns a list of tuples in which the first element in each tuple is a unique name string, the second is a list of possible ID matches, and the third is a list of prior probabilities for each match. For the time being, those prior probailities will all be set equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def build_aliases(altnames, ids):\n",
    "    '''\n",
    "    Generates descriptions for each person\n",
    "        altnames: a list of alternate names from generate_altnames        \n",
    "        ids: a list of identifiers from build_kb_seed\n",
    "\n",
    "        returns: a list of tuples in which the first element is a unique name string, \n",
    "        the second is a list of possible ID matches, \n",
    "        and the third is a list of prior probabilities for each match\n",
    "    '''\n",
    "    \n",
    "    unames = []\n",
    "    poss_matches = []\n",
    "    \n",
    "    for item in altnames:\n",
    "        for name in item:\n",
    "            if name not in unames:\n",
    "                unames.append(name)\n",
    "                \n",
    "    for i in range(len(unames)):\n",
    "        temp = []\n",
    "        for j in range(len(altnames)):\n",
    "            if unames[i] in altnames[j]:\n",
    "                temp.append(ids[j])\n",
    "        poss_matches.append(temp)\n",
    "                \n",
    "    probs = []\n",
    "    \n",
    "    for k in range(len(unames)):\n",
    "        probs.append([(1 / len(poss_matches[k]))] * len(poss_matches[k]))\n",
    "        \n",
    "    \n",
    "    return unames, poss_matches, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit testing: `generate_altnames` and `build_aliases`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests these functions using dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Daniel', 'Dan', 'Daniel Genkins'], ['Dan', 'Daniel', 'Daniel Genkins'], ['Tyrion'], ['Daniel Genkins', 'Daniel', 'Dan']]\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "#expected output: [[\"Daniel\", \"Dan\", \"Daniel Genkins\"], [\"Dan\", \"Daniel\", \"Daniel Genkins\"], [\"Tyrion\"], \n",
    "#[\"Daniel Genkins\", \"Daniel\", \"Dan\"]]\n",
    "\n",
    "list_of_names = [\"Daniel\", \"Dan\", \"Tyrion\", \"Daniel Genkins\"]\n",
    "altnames = generate_altnames(list_of_names)\n",
    "print(altnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Daniel', 'Dan', 'Daniel Genkins', 'Tyrion'],\n",
       " [[1, 2, 4], [1, 2, 4], [1, 2, 4], [3]],\n",
       " [[0.3333333333333333, 0.3333333333333333, 0.3333333333333333],\n",
       "  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333],\n",
       "  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333],\n",
       "  [1.0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no_test\n",
    "#expected output\n",
    "#unames = ['Daniel', 'Dan', 'Daniel Genkins', 'Tyrion']\n",
    "#poss_matches = [[1, 2, 4], [1, 2, 4], [1, 2, 4], [3]]\n",
    "#probs = [[.333, .333, .333], [.333, .333, .333], [.333, .333, .333], [1]]\n",
    "\n",
    "list_of_ids = [1, 2, 3, 4]\n",
    "build_aliases(altnames, list_of_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_kb` combines the helper functions defined above to build a spaCy knowledge base, which can then be saved using `save_kb`\n",
    "\n",
    "The loops that build the knowledge base are adapted from https://github.com/explosion/projects/blob/master/nel-emerson/scripts/el_tutorial.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def create_kb(peopleCSV, ppeCSV, xml_df):\n",
    "    '''\n",
    "    Creates a spaCy knowledge base\n",
    "        peopleCSV: a csv containing all people who appear in a given volume\n",
    "        ppeCSV: a csv linking these people to specific events\n",
    "        xml_df: dataframe built by the parse_xml function from ssda.xml_parser\n",
    "\n",
    "        returns: a spaCy knowledge base\n",
    "    '''\n",
    "    \n",
    "    #using helper functions from above to build all required pieces for kb\n",
    "    ids, names, refs, freqs = build_kb_seed(peopleCSV, ppeCSV)\n",
    "    descriptions = generate_descriptions(names, refs, xml_df)\n",
    "    altnames = generate_altnames(names)\n",
    "    unames, poss_matches, probs = build_aliases(altnames, ids)\n",
    "    \n",
    "    nlp = spacy.load(\"es_core_news_md\")\n",
    "    #just grabbed md for convenience since it also includes vectors\n",
    "    #lg is likely a better option if this yields meaningful results\n",
    "    \n",
    "    kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=50)\n",
    "    #not quite sure what entity vector length is, but I believe that it's defined by the model loaded above\n",
    "    \n",
    "    for i in range(len(ids)):\n",
    "        desc_doc = nlp(descriptions[i])\n",
    "        desc_enc = desc_doc.vector\n",
    "        kb.add_entity(entity=ids[i], entity_vector=desc_enc, freq=freqs[i])        \n",
    "    \n",
    "    for j in range(len(unames)):\n",
    "        kb.add_alias(alias=unames[j], entities = poss_matches[j], probabilities = probs[j])                \n",
    "        \n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def save_kb(kb, output_dir):\n",
    "    \n",
    "    invalid_dir = False    \n",
    "   \n",
    "    #confirms that desired directory is valid, and creates it if it doesn't already exist\n",
    "    if os.path.isfile(output_dir):\n",
    "        invalid_dir = True\n",
    "    elif not os.path.isdir(output_dir):\n",
    "        os.mkdir(output_dir)    \n",
    "    \n",
    "    #saves knowledge base\n",
    "    if not invalid_dir:\n",
    "        kb_path = output_dir + \"\\\\kb\"\n",
    "        kb.dump(kb_path)        \n",
    "        print(\"Saved KB to\", kb_path)\n",
    "\n",
    "        vocab_path = output_dir + \"\\\\vocab\"\n",
    "        kb.vocab.to_disk(vocab_path)\n",
    "        print(\"Saved vocab to\", vocab_path)\n",
    "        \n",
    "    return kb_path, vocab_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit testing: `create_kb` and `test_kb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved KB to C:\\Users\\Daniel Genkins\\kb_test\\kb\n",
      "Saved vocab to C:\\Users\\Daniel Genkins\\kb_test\\vocab\n",
      "\n",
      "Entities in the KB: ['P009-001417', 'P009-001339', 'P009-001583', 'P009-001370', 'P009-001592', 'P009-001296', 'P009-001241', 'P009-001240', 'P009-001375', 'P009-001325', 'P009-001578', 'P009-001577', 'P009-001368', 'P009-001435', 'P009-001363', 'P009-001520', 'P009-001359', 'P009-001396', 'P009-001418', 'P009-001488', 'P009-001354', 'P009-001387', 'P009-001242', 'P009-001426', 'P009-001567', 'P009-001288', 'P009-001312', 'P009-001383', 'P009-001425', 'P009-001286', 'P009-001561', 'P009-001310', 'P009-001458', 'P009-001265', 'P009-001268', 'P009-001302', 'P009-001326', 'P009-001439', 'P009-001357', 'P009-001602', 'P009-001519', 'P009-001596', 'P009-001595', 'P009-001253', 'P009-001351', 'P009-001450', 'P009-001423', 'P009-001616', 'P009-001401', 'P009-001335', 'P009-001476', 'P009-001255', 'P009-001275', 'P009-001397', 'P009-001518', 'P009-001340', 'P009-001249', 'P009-001420', 'P009-001529', 'P009-001394', 'P009-001474', 'P009-001437', 'P009-001610', 'P009-001598', 'P009-001292', 'P009-001348', 'P009-001603', 'P009-001402', 'P009-001408', 'P009-001372', 'P009-001412', 'P009-001475', 'P009-001495', 'P009-001365', 'P009-001413', 'P009-001553', 'P009-001535', 'P009-001422', 'P009-001504', 'P009-001284', 'P009-001328', 'P009-001515', 'P009-001601', 'P009-001390', 'P009-001297', 'P009-001590', 'P009-001480', 'P009-001389', 'P009-001309', 'P009-001360', 'P009-001346', 'P009-001571', 'P009-001361', 'P009-001510', 'P009-001391', 'P009-001308', 'P009-001283', 'P009-001503', 'P009-001587', 'P009-001373', 'P009-001427', 'P009-001299', 'P009-001605', 'P009-001617', 'P009-001305', 'P009-001576', 'P009-001548', 'P009-001501', 'P009-001254', 'P009-001443', 'P009-001331', 'P009-001543', 'P009-001471', 'P009-001293', 'P009-001507', 'P009-001274', 'P009-001533', 'P009-001508', 'P009-001560', 'P009-001469', 'P009-001516', 'P009-001376', 'P009-001261', 'P009-001404', 'P009-001527', 'P009-001532', 'P009-001493', 'P009-001539', 'P009-001594', 'P009-001272', 'P009-001352', 'P009-001414', 'P009-001267', 'P009-001279', 'P009-001273', 'P009-001467', 'P009-001470', 'P009-001419', 'P009-001545', 'P009-001416', 'P009-001544', 'P009-001459', 'P009-001453', 'P009-001606', 'P009-001238', 'P009-001483', 'P009-001410', 'P009-001371', 'P009-001366', 'P009-001619', 'P009-001393', 'P009-001428', 'P009-001556', 'P009-001457', 'P009-001609', 'P009-001349', 'P009-001491', 'P009-001492', 'P009-001593', 'P009-001395', 'P009-001329', 'P009-001424', 'P009-001455', 'P009-001541', 'P009-001530', 'P009-001570', 'P009-001239', 'P009-001531', 'P009-001369', 'P009-001377', 'P009-001440', 'P009-001316', 'P009-001337', 'P009-001447', 'P009-001546', 'P009-001307', 'P009-001381', 'P009-001479', 'P009-001324', 'P009-001584', 'P009-001611', 'P009-001243', 'P009-001403', 'P009-001237', 'P009-001615', 'P009-001358', 'P009-001236', 'P009-001445', 'P009-001442', 'P009-001266', 'P009-001481', 'P009-001511', 'P009-001599', 'P009-001248', 'P009-001374', 'P009-001432', 'P009-001334', 'P009-001614', 'P009-001342', 'P009-001295', 'P009-001468', 'P009-001613', 'P009-001588', 'P009-001234', 'P009-001434', 'P009-001367', 'P009-001262', 'P009-001263', 'P009-001429', 'P009-001235', 'P009-001304', 'P009-001446', 'P009-001303', 'P009-001466', 'P009-001460', 'P009-001291', 'P009-001319', 'P009-001433', 'P009-001290', 'P009-001513', 'P009-001573', 'P009-001568', 'P009-001285', 'P009-001555', 'P009-001244', 'P009-001281', 'P009-001252', 'P009-001313', 'P009-001338', 'P009-001314', 'P009-001461', 'P009-001464', 'P009-001341', 'P009-001301', 'P009-001431', 'P009-001536', 'P009-001463', 'P009-001604', 'P009-001347', 'P009-001392', 'P009-001517', 'P009-001618', 'P009-001509', 'P009-001485', 'P009-001388', 'P009-001278', 'P009-001287', 'P009-001400', 'P009-001436', 'P009-001462', 'P009-001378', 'P009-001449', 'P009-001333', 'P009-001575', 'P009-001282', 'P009-001608', 'P009-001345', 'P009-001574', 'P009-001385', 'P009-001317', 'P009-001569', 'P009-001558', 'P009-001251', 'P009-001258', 'P009-001528', 'P009-001441', 'P009-001256', 'P009-001484', 'P009-001537', 'P009-001487', 'P009-001355', 'P009-001398', 'P009-001271', 'P009-001586', 'P009-001306', 'P009-001490', 'P009-001542', 'P009-001318', 'P009-001456', 'P009-001521', 'P009-001406', 'P009-001300', 'P009-001612', 'P009-001311', 'P009-001247', 'P009-001554', 'P009-001572', 'P009-001585', 'P009-001364', 'P009-001557', 'P009-001276', 'P009-001540', 'P009-001344', 'P009-001245', 'P009-001597', 'P009-001411', 'P009-001409', 'P009-001298', 'P009-001549', 'P009-001496', 'P009-001451', 'P009-001589', 'P009-001384', 'P009-001438', 'P009-001506', 'P009-001382', 'P009-001550', 'P009-001566', 'P009-001489', 'P009-001264', 'P009-001547', 'P009-001494', 'P009-001505', 'P009-001430', 'P009-001353', 'P009-001356', 'P009-001379', 'P009-001473', 'P009-001482', 'P009-001452', 'P009-001600', 'P009-001407', 'P009-001380', 'P009-001477', 'P009-001472', 'P009-001399', 'P009-001591', 'P009-001294', 'P009-001478', 'P009-001552', 'P009-001448', 'P009-001343', 'P009-001538', 'P009-001502', 'P009-001327', 'P009-001330', 'P009-001386', 'P009-001534', 'P009-001246', 'P009-001415', 'P009-001551', 'P009-001257', 'P009-001559', 'P009-001454', 'P009-001421', 'P009-001444', 'P009-001486', 'P009-001514', 'P009-001250']\n",
      "\n",
      "Aliases in the KB: ['Francisco Almeyda', 'Asencion Suares', 'Maria Dolores V#a', 'Manuel Dias', 'Josefa B#', 'Raf# #ales', 'Josef Antonio', 'Daniel Jorge', 'Juan de Dios Saldi#ar', 'Eugenio Gomes', 'Juan Nepomucemo de Mesa', 'Antonia Maria Mandinga', 'Juana Jo# #', 'Pedro Villalobos', 'Maria de los Ynocentes Perdono', 'Maria Rosario #menteros', 'Lorenso Carabali', 'Jose Maria de Leira', 'Josef Dolores Amador', 'Maria Merced', 'Jose Perez', 'Eulalia Miranda', 'Petrona Sandoval', 'Pedro Congo', 'Antonio Gonsales', 'Jorge Villar', 'Josef San#', 'Pablo Ayende', 'Rafaela de Olivo', 'Juan Nepomuceno Diez', 'Ana Muñoz', 'Josefa del Castillo', 'Bartolomé Huer#', 'Luis Galindo', 'Juan #po', 'Maria Montana', 'Maria del Carmen Barrios', 'Antonio Arnaldo', 'Josef Martin', 'Gabriela Josefa de la Trinidad Rondon', 'Rafael de Ayala', 'Ma# Muñoz', 'Maria de la Presentacion Estrada', 'Maria Concepcion Otero', 'Petrona Samiñon', 'Rosa Maria Macua', 'Felipe Ayala', 'Francisco Maria Lucumi', '#na de Dios Gelab#r', 'Francisco Peñalber', 'Manuela Antonia', 'Maria Barb#ral Men#cal', 'Maria del Carmen Fernandez', 'Gabriel Garcia', 'Jose Maria Otero', 'Josef de la Cruz Yglesias', 'Maria Petrona', 'Maria Luisa Rodrigues', 'Angela Barboso', 'Teodoro Gamarra', 'Josef Maria Fernandez', 'Tomas Geraldo', 'Monserrate Basta#', 'Maria Brigida Armo#teros', 'Maria Candelaria Valdes', 'Pedro Pablo Garola', 'Bonifacio Lucumi', 'Josef de Jesus Baesa', 'Tomas', 'Pedro Martin', 'Cornelio Juan Bautista de Jesus Nazareno Manrreza', 'Josef Maria Parra', 'Ramona Gamarra', 'Maria del Moncerrate', 'Maria del Rosario Susarte', 'Mariana de Jesus Ganga', 'Josef Leandro Pedroso', 'Jose Camilo Montenegro Crespo', 'Maria del Rosario Ramires', 'Andres Cascales', 'Ygnacio Josef', 'Tomas Jose Gangá', 'Maria Dolores Gonsales', 'Juan Bautista Galailena', '# Herrera', 'Maria Antonia Lieche', 'Blas de Mesa', 'Yrpiano Pedrozo', 'Manuel Pio Muñoz', 'Jacinto Beltran', 'Maria Lopez', 'Ana Josefa Cayetana Renova', 'Manuel de la Torre Machado y Castillo', 'Ursula Cedano', 'Maria Monserrate Mina', 'Juan de Dios Y#', 'Josef Miranda', 'Maria Dolores Salazar', 'Tomas del Calvo', 'Pedro Samiñon', 'Juan Nepomuceno Escobal', 'Josef Maria Rangel', 'Martin de A# y Herrera', 'Maria de los Dolores', 'Maria Francisca', 'Rosalia Sanches', 'Lorenso Laguardia', 'Juan Jose #ancor#', 'Maria del Rosario de Hita', 'Placido Josef del Rosario Acosta', 'Maria Decideria Primero', 'Maria Concepcion Rodrigues', 'Andres Josef Aguiar', 'Ygnacio de Herrera', 'Maria Elena', 'Catalina de L#a', 'Gabriel Carabali', 'Apolonia Rodrigues', 'Maria Concepcion de la Torre', 'Francisco de Loria', 'Maria Trinidad Muñoz', 'Juan Bautista', 'Rosa Garcia', 'Manuel Silbestre', 'Nicolas Jayme', 'Pedro Ruiz', 'Felipa Carvajal', 'Felix Quixano', 'Gertrudis', 'Felipe de Bera', 'Juana Fornari', 'Maria Lor#', 'Maria del Carmen Carabali', 'Agustin Serecio', 'Margarita Francisca Hernandez', 'Gregoria Sotolongo', 'Josef Tomas Jaresin', 'Francisco #oso y Ba#', 'Merced Carbelo', 'Jose Nicolas Carabali', 'Josef Rafael Morales', 'Carlos Armenteros', 'Josefa Velasco', '#erquez', 'Jose de Jesus Cabrera', 'Antonio Garcia', 'Juan de Dios Henrique', 'Antonio', 'Pedro Garcia', 'Catalina Cabral', 'Maria Josefa Catarina Gomes', 'Maria Francisca Valdes', 'Tomasa #gui#lo', 'Juan Francisco Congo', 'Bernarda de la Cruz', 'Rita Gonsales', 'Antonio Vinolo', 'Jose Alexo Rosales', 'Francisco Delgado', 'Maria Gregoria', 'Maria del Rosario Acosta', 'Manuel Mandinga', 'Jose Quiróz', 'Jose Pio', 'Francisco Serecio', 'Fr#.co Q#edo', 'Maria del Rosario', 'Getrudis de Zuninga', 'Jose Toribio Quiróz', 'Josef Congo', 'Josef Margarito Suares', 'Juana Josefa Gailan', 'Bernardo de la Torre', 'Ygnacio', 'Juan Escobar', 'Blas Sandoval', 'Antonia Abad Aparecio', 'Josefa Quixano', 'Simona', 'Josef Placido Gonzales', 'Maria de la Concepcion Chapus', 'Maria Rosario de Soto', 'Jose de Flores #', 'Juan Nepomuceno Carabali', 'Manuel de Jesus', 'Maria Cecilia Cedano', 'Felix Jose Desiderio Chapuz', 'Maria Monserrate Beltran', 'Marcelino Congo', 'Maria de la Resurreccion Gonsales', 'Teresa Arostegui', 'Josef Echavarria', 'Susana', 'Jose de la Cruz Villafranca', 'Maria de la Lus Dias', 'Francisco Zemea', 'Maria Bernarda de Bera', 'Maria del Rosario Perez', 'Jacinto Cabral', 'Maria de Jesus de los Dolores', 'Catalina Fernandez', 'Tomasa Maria', 'Maria Josefa Luisa', 'Juana de Jesus Garcia', 'Pregoria Pedrozo', 'Francisco #ra', 'Rafaela', 'Maria Francisca de Armas', 'Manuela Otero', 'Jose Francisco Gamboa', 'Maria Lorenza Diaz', 'Maria Salome Duarte', 'Antonia Maria Diaz', 'Antonio Vinelo', 'Manuela de Jesus Ortiz', 'Tomasa', 'Francisca Almeyda', 'Maria Ramires', 'Maria Antonia Pomares', 'Francisco Xavier Viera', 'Maria Luisa Cabral', 'Maria Candelaria', 'Paula de la Cruz', 'Maria Pilar Verasaluce', 'Rafael', 'Rafael Valdes', 'José Maria Eusebio Ponce', 'Pelegrina Sotomayor', 'Vicente Arara', 'Marcelino Joaquin', 'Apolonia Gangá', 'Maria Francisca Laguardia', 'Francisco Gallardo', 'Maria Merced Chacon', 'M# Valdés', 'Ju# #drazo', 'Maria del Rosario Gomes', 'Teresa Ravelo', 'Juan Josef de Hita', 'Manuel Cedano', 'Maria Josefa Carabali', 'Antonio Jose de la Cruz', 'Clemente Bricamo', 'Lu# # Seguera', 'Teresa Caballero', 'Josefa Maria Hernandez', 'Marcelina Marines', 'Josef Rafael', 'Paulina #rer', 'Manuel Apesechea', 'Antonia Samiñon', 'Francisco de Paula Congo', 'Pedro Josef Ynte#', 'Matias Medan', 'Rafael Aparecio', 'Jose Desiderio', 'Jose Sanchez', 'Guillermo B#', 'Maria Merced Marot#', 'Diego #gui#lo', 'Maria Luisa', 'Jacinto Ganga', 'Maria de la Cruz', 'Petrona', 'Maria de las Nieves', 'Rosa Congo', 'Tomas Mariano Gonsales', 'Francisco Quivos', 'Dolores Joaquina', 'Maria Casilda', 'Manuel Valdes', 'Maria de Regla', 'Dionicia Castellanos', 'Maria de la Luz de Soto', 'Rafael Gonzales', 'Merced Acosta', 'Gregorio Barrios', 'Maria Josefa Perez', 'Maria Antonia Carabali', 'Josef Maria', '# de la Concepcion Herrera', 'Francisco Ruiz', 'Antonio Abad Villegas', 'Jose Casilda', 'Josef Ambrocio', 'Yginio Dias', 'Maria Josefa', 'José de Rivas', 'Maria Trinidad Eligio', 'Gavino Rodrigues', 'Fermin Almirante', 'Luis Congo', 'Ysabel Gallardo', 'Santiago de Lu#', 'Josef Maria Carabali', 'Micaela Josefa Alvarado', 'Francisco Toscano', 'Ana Joaquina del Rey', 'Maria Dolores Rosas', 'Rafaela Antonia Muñoz', 'Miguel Josef', 'Josef Rafael Peres', 'Enrique de Raxas', 'Maria de la # Calvo', 'Francisco #', 'Francisco Cortinas', 'Josef Rafael Arango', 'Juan de Dios Carabali', 'Josef Maria Chacon', 'Josef Miguel Congo', 'Josef de la Trinidad', 'Josef Ramon Petit', 'Juan Pedro Her#', 'Agustin', 'Diego Mandinga', 'Maria del Rosario Ruiz', 'Rafael Carabali', 'Francisco Xavier Gonzales', 'Antonia Gamboa', 'Mariana Arriola', 'Rafael Morales', 'Josef Nicolas', 'Juana Leandra', 'Maria de Regla #', 'Manuela Sanches', 'Antonio Rangel', 'Maria de la Soledad Olibos', 'Maria de la Luz', 'Tomas Arostegui', 'Tomasa Hermosa', 'Soledad Salamanca', 'Josef Francisco Gondra', 'Josef Ynocen# Beytia', 'Mariano Valdes', 'Maria Dolores Pacheco']\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "test_kb = create_kb(\"FourPeople.csv\",\"FourPeoplePerEntry.csv\", parse_xml(\"four.xml\"))\n",
    "save_kb(test_kb, \"C:\\\\Users\\\\Daniel Genkins\\\\kb_test\")\n",
    "\n",
    "print(\"\\n\" + f\"Entities in the KB: {test_kb.get_entity_strings()}\" + \"\\n\")\n",
    "print(f\"Aliases in the KB: {test_kb.get_alias_strings()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building training data for modelling with entity linker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is not directly related to those above, but rather transforms Spatial Historian data to the format required in order to train a spaCy model with entity linking (so it *is* indirectly related in that all of the functions in this notebook are required in order to execute the full entity linking training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def get_poss_ids(sources, folio_id, events, ent_no, people, entity, ids, names):\n",
    "    '''\n",
    "    Helper function to streamline build_el_training_data\n",
    "    \n",
    "    Takes a variety of inputs and returns a list of possible IDs\n",
    "    '''\n",
    "\n",
    "    poss_ev = []\n",
    "    poss_ids = []    \n",
    "    \n",
    "    #identify possible events\n",
    "    for j in range(len(sources)):\n",
    "        if sources[j] == folio_id:            \n",
    "            poss_ev.append(events[j])\n",
    "    \n",
    "    #isolate specific event    \n",
    "    event = poss_ev[ent_no - 1]    \n",
    "            \n",
    "    #identify attendees of specific event\n",
    "    for k in range(len(events)):\n",
    "        if events[k] == event:            \n",
    "            poss_ppl = people[k]            \n",
    "   \n",
    "    #find all possible IDs for each entity mention   \n",
    "    for person in poss_ppl:\n",
    "        for l in range(len(ids)):            \n",
    "            if (person == ids[l]) and (entity == names[l]):                                      \n",
    "                poss_ids.append(ids[l])\n",
    "                    \n",
    "            \n",
    "    #catches corner cases where names are garbled\n",
    "    if len(poss_ids) == 0:\n",
    "        for person in poss_ppl:\n",
    "            for l in range(len(ids)):\n",
    "                if (person == ids[l]) and (entity in names[l]):                   \n",
    "                    poss_ids.append(ids[l])   \n",
    "                        \n",
    "    return poss_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def build_el_training_data(train_df, ppeCSV, peopleCSV):\n",
    "    '''\n",
    "    Creates a spaCy knowledge base\n",
    "        train_df: dataframe containing portion of data earmarked for training by ssda.split_data\n",
    "        peopleCSV: a csv containing all people who appear in a given volume\n",
    "        ppeCSV: a csv linking these people to specific events        \n",
    "\n",
    "        returns: training data formatted for use with a spaCy entity linking model\n",
    "    '''\n",
    "    \n",
    "    #turn df into lists for ease of manipulation\n",
    "    entry_texts = train_df[\"text\"].tolist()    \n",
    "    starts = train_df[\"start\"].tolist()\n",
    "    ends = train_df[\"end\"].tolist()\n",
    "    entities = train_df[\"entity\"].tolist()\n",
    "    folio_ids = train_df[\"fol_id\"].tolist()\n",
    "    entry_ids = train_df[\"entry_no\"].tolist()\n",
    "    \n",
    "    #set up data structures\n",
    "    u_txt = []\n",
    "    dicts = []   \n",
    "    #a list of tuples in which the first element in each tuple is an entry text and the second is a dictionary of dictionaries\n",
    "    el_inp = []\n",
    "    \n",
    "    #read in CSVs\n",
    "    with open(peopleCSV, encoding=\"utf-8\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        ids = []\n",
    "        names = []\n",
    "        first = True\n",
    "        for row in csvreader:\n",
    "            if first:\n",
    "                first = False\n",
    "                continue\n",
    "            ids.append(row[0])\n",
    "            names.append(row[1])\n",
    "            \n",
    "    with open(ppeCSV, encoding=\"utf-8\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        sources = []\n",
    "        people = []\n",
    "        events = []\n",
    "        first = True\n",
    "        for row in csvreader:\n",
    "            if first:\n",
    "                first = False\n",
    "                continue\n",
    "            sources.append(row[1][-4:])\n",
    "            events.append(row[0][-3:])\n",
    "            ppl = []\n",
    "            ppl.append(row[3])\n",
    "            for attendee in row[4].split(';'):\n",
    "                person = attendee[attendee.find(\"P0\"):]\n",
    "                if person not in ppl:\n",
    "                    ppl.append(person)\n",
    "            for other in row[5].split(';'):\n",
    "                person = other[other.find(\"P0\"):]\n",
    "                if (other != '') and (person not in ppl):                   \n",
    "                    ppl.append(person)\n",
    "            people.append(ppl)    \n",
    "      \n",
    "    #loop through each entity reference and attach to appropriate entity text\n",
    "    for i in range(len(entry_texts)):        \n",
    "        folio_id = folio_ids[i]\n",
    "        ent_no = int(entry_ids[i][-1:])\n",
    "        start = int(starts[i])\n",
    "        end = int(ends[i])\n",
    "        entry_text = entry_texts[i]\n",
    "        entity = entities[i]\n",
    "        \n",
    "        #build output components\n",
    "        poss_ids = get_poss_ids(sources, folio_id, events, ent_no, people, entity, ids, names)\n",
    "        prob = 1 / len(poss_ids)\n",
    "        \n",
    "        #combine output components\n",
    "        if entry_text in u_txt:\n",
    "            pos = u_txt.index(entry_text)\n",
    "            dicts[pos][start, end] = {}\n",
    "            dicts[pos][start, end][poss_ids[0]] = prob\n",
    "        else:\n",
    "            u_txt.append(entry_text)\n",
    "            new_dict = {(start, end): {poss_ids[0]: prob}}\n",
    "            dicts.append(new_dict)\n",
    "        for m in range(1, len(poss_ids)):\n",
    "            dicts[pos][start, end][poss_ids[m]] = prob           \n",
    "            \n",
    "    #append output components to return value\n",
    "    for x in range(len(u_txt)):\n",
    "        el_inp.append((u_txt[x], {\"links\": dicts[x]}))            \n",
    "    \n",
    "    return el_inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit testing: `build_el_training_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('N. 46 Jose Camilo Montenegro Crespo, y Maria Concep.n Otero. En la ciudad de la Habana en veinte y dos de # de mil ochocientos catorce años Habiendo precedido las dilig# de estilo, y leidose las tres canonicas proclamas en tres dias festivos sin resultar impedimen# Yo don Jacinto Beltran presb.o encargrado por # de la Iglesia Auxiliar del Santo Angel Custodio: Desp# y vele segun el Ritual Romano á Jose Camil# Montenegro Crespo, hijo natural de Juan # po, natural y vecino de esta ciudad y feligre# 13.  #ardo libre; y á Maria de la Concep.n Otero, de ig.l clase, y de la prop.a naturalidad y vecindario, hija lexitima de Jose Maria Otero, y de Manuela de Jesus Ortiz; habiendoles preguntado tube por respuesta su mutuo consentim.to confesaron, comulgaron, fueron examinados en la Doctrina Cristiana, siendo #adrinos Jorge Villar, y Manuela Otero, y #tigos don Pedro Garcia, y don Manuel Valdes, y lo firmé. Jacinto Beltran y #quero ', {'links': {(448, 457): {'P009-001575': 1.0}, (864, 876): {'P009-001448': 1.0}, (6, 35): {'P009-001572': 1.0}, (884, 897): {'P009-001316': 1.0}, (911, 926): {'P009-001574': 1.0}}})\n",
      "('N. 42. Pedro Pablo Garcíarola, y Gabriela Josefa de la Trinidad Rondon. En la ciudad de la Habana en # de Diciembre d# mil ochocientos trece años. Hab#dose publicado las tres canonicas amonestaciones # dias festivos sin resultar impedimento Yo. Licenciado Don# Andrés Cascales Beneficiado de la Iglesia Auxiliar del Santo Angel Custodio: Despose segun Rito E~cco. á Pedro Pablo Garola, natural de esta ciudad y vecino de esta feligresia hijo legítimo de Jose, y de Maria Casilda; y Gabriela Josefa de la Trin#dad Ron # de la prop.a naturalidad y vecindario, hija le gitima de Rafael, y de Antonia Abad Aparecio pardos libres habiendoles preguntado, tube por respuesta su mutuo consentimiento; confesaron, comulgaron fueron examinados en la Doctrina Cristiana, y l# p#vine se velen # tiempo habil, fueron sus padres Don Santiago de Lu#, y Doña Monserrate Basta#, y t#tigos Don Pedro Garcia, y Don Manuel Valdes, y lo firmé Licenciado Andres Cascales ', {'links': {(933, 948): {'P009-001234': 1.0}, (843, 860): {'P009-001549': 1.0}, (465, 478): {'P009-001545': 1.0}, (819, 834): {'P009-001548': 1.0}, (589, 610): {'P009-001547': 1.0}, (876, 888): {'P009-001448': 1.0}, (33, 70): {'P009-001543': 1.0}}})\n",
      "('N 32 Josef Ambrocio criollo, y Maria del Rosario # Gonzales En la ciudad de la Havana, en nueve del Agosto de mil ocho#os y trece años, haviendo precedido las tres #nicas amonesta ciones publicadas en tres dias festivos, sin resultar impe dimento, Yo Licenciado D Andres Cascales Beneficiado de la Iglesia Auxiliar del Santo Angel Custodio de esta dha ciudad, case y vele ritualmente a Josef Ambrocio Criollo, hi# legítimo de Josef Francisco Gondra, y de Paula de la Cruz, y a Maria del Rosario, de esta naturalidad, hija legítima de Francisco de Paula Congo, y de Maria Merced Criolla, negros esclavos del Don D Rafael Gonzales, y dhos contrayentes confesaron y comulgaron, fueron examminados en la doctrina cristiana, siendo padrinos Francisco Toscano y Maria Merced Marot#, testigos D Pedro Garcia, y D Manuel Valdes, y lo firme Licenciado Andres Cascales ', {'links': {(534, 558): {'P009-001470': 1.0}, (386, 400): {'P009-001466': 1.0}, (31, 48): {'P009-001467': 1.0}, (565, 577): {'P009-001471': 1.0}, (426, 448): {'P009-001468': 1.0}, (477, 494): {'P009-001467': 1.0}, (843, 858): {'P009-001234': 1.0}, (613, 628): {'P009-001474': 1.0}, (756, 768): {'P009-001471': 1.0}, (5, 19): {'P009-001466': 1.0}, (806, 819): {'P009-001316': 1.0}, (788, 800): {'P009-001448': 1.0}}})\n",
      "('N. 49. Cornelio Juan Bautista Manrreza, y Maria Cande#ria Valdes En la ciudad de la Habana en veinte y tres de M# yo de mil ochocinetos catorce años Habiendo prece dido las diligencias de estilo y leidose las tres canon# cas proclam# en tres dias festivos sin resultar impedimento Yo Don Jacinto Beltran Presbítero. enca# por San San Y. de la Iglesia Auxiliar del Santo Angel Cus# #ta en la Hermita de N. Señora. de Mo# 14  te: Desposé y vele segun el Ritual Romano á Cornelio Juan Bautista de Jesus Nazareno Manrreza, pardo libre natural y vecino del part.do de San Julian de Guines, hijo lex^mo de Diego, y de Tomasa #gui#lo; y á Maria Candelaria Valdes parda libre, de esta feligresia: habiendoles preguntado tube por respuesta su mutuo consentim.to confesaron, comulgaron, fueron examinados en la Doctrina Cr#iana, siendo padrinos Don Fermin Almirante, y Doña Maria Dolores Rosas, y testigos Don Pedro Garcia, y Don Mariano Valdes, y lo firmé. Jacinto Beltran ', {'links': {(632, 655): {'P009-001598': 1.0}, (839, 855): {'P009-001601': 1.0}, (468, 517): {'P009-001597': 1.0}, (864, 883): {'P009-001602': 1.0}, (612, 626): {'P009-001600': 1.0}, (920, 934): {'P009-001426': 1.0}, (288, 303): {'P009-001574': 1.0}}})\n",
      "('N. 41. Felix Jose Desiderio Chapuz y Maria de la Resurreccion Gonsales En la ciudad de la Habana en seis #iciembre de mil ochocientos trece años. Habiendose publicado las tres canonicas amonestaciones, en tres dias festivos sin resultar impedimento Yo Licenciado Don Andres Cascales Beneficiado de la Iglesia Auxiliar del Santo Angel Custodio. Desposé segun Rito E~cco. á Felix Jose Desiderio Chapuz, natural de esta ciudad, y vec.no de la #, felig.a de la Parroquial de Guadalupe, extramuros de esta ciudad, hijo legítimo de Felix, y de Josefa Quixano, pardos libres, y á Maria de la Resurreccion Gonsales, de la prop.a naturalidad, y de esta felig.a hija legitima de Jose, y de Maria Josefa Perez, pardos libres: habiendoles preg.do tube por r#uesta su mutuo consentimiento: confesaron, comulgaron, fueron examinados en la Doctrina Cristiana, y les previne se velen en tiempo habil, siendo testigos Don Pedro Garcia, y Don Manuel Valdés y padrinos, Año# Garcia, y Maria de la Concep.n Chapus, y lo firme. Lic.# A# C# ', {'links': {(538, 552): {'P009-001537': 1.0}, (37, 70): {'P009-001535': 1.0}, (680, 698): {'P009-001539': 1.0}, (372, 399): {'P009-001534': 1.0}, (573, 606): {'P009-001535': 1.0}, (7, 34): {'P009-001534': 1.0}, (267, 282): {'P009-001234': 1.0}, (905, 917): {'P009-001448': 1.0}}})\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "# load and create df from xml\n",
    "xml_df = parse_xml(\"four.xml\")\n",
    "\n",
    "# Create entity df from entity csvs\n",
    "ent_df = entity_df_maker(\"FourPeoplePerEntry.csv\", \"FourPeople.csv\")\n",
    "\n",
    "# Put these two dfs together and with entity span info\n",
    "collated_df = collate_frames(xml_df, ent_df)\n",
    "\n",
    "#train/test split\n",
    "train, test = split_data(collated_df)\n",
    "\n",
    "el_train_data = build_el_training_data(train, \"FourPeoplePerEntry.csv\", \"FourPeople.csv\")\n",
    "\n",
    "#print output for visual inspection\n",
    "for i in range(5):\n",
    "    print(el_train_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 10-process-data.ipynb.\n",
      "Converted 11-entity-corpus-dataframe.ipynb.\n",
      "Converted 12-ssda-xml-parser.ipynb.\n",
      "Converted 20-data-exploration.ipynb.\n",
      "Converted 30-dataset-generation.ipynb.\n",
      "Converted 31-collate-xml-entities-spans.ipynb.\n",
      "Converted 32-gen-spacy-input.ipynb.\n",
      "Converted 33-split-data.ipynb.\n",
      "Converted 34-add-entities.ipynb.\n",
      "Converted 35-entity-linker.ipynb.\n",
      "Converted 40-features-models-reports.ipynb.\n",
      "Converted 41-generic-framework-for-spacy-training.ipynb.\n",
      "Converted 42-testing-full-pipeline.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#no_test\n",
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
